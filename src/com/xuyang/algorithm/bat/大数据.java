package com.xuyang.algorithm.bat;

/**
 * @Auther: allanyang
 * @Date: 2020/1/13 15:26
 * @Description:
 */
public class 大数据 {

    // 512M的内存空间大概是2^32次方比特位的大小。

    // 1.Map-Reduce
    /**
     * 1.Map阶段 ： 把大任务分成子任务
     * 2.Reduce阶段 ： 子任务并发执行，然后合并结果。
     *
     * 注意点：
     * 1.备份的考虑，分布式存储的设计细节，以及容灾策咯。
     * 2.任务分配策略与任务进度跟踪的细节设计，节点状态的呈现。
     * 3.多用户权限控制。
     */

    // 2.常见海量处理题目解题关键
    /**
     * 1.分而治之。通过哈希函数将大任务分流到机器，或分流到小文件。
     * 2.常用的hashMap或bitmap。
     */

    // 3.对10亿个IPV4地址进行排序，每个ip只会出现一次。
    /**
     * 将ip转为无符号整数。然后排序整数再转为IPV4。
     *
     * 利用bitmap，先转换后描黑，然后遍历bitmap，再遍历的过程中将描黑的再转成ipv4。
     */

    // 4.对10亿人年龄进行排序。
    /**
     * 桶排序思想。分布为0~200个桶。
     */

     // 5.有一个包含20亿个全是32为整数的大文件，再其中找到出现次数最多的数，但是内存限制只有2G.
    /**
     * hashmap来计算，很可能内存溢出。
     *
     * 1.首先利用hashmap来将大文件进行分流，分流到16个小文件上，相同的数肯定再同一个文件内部。
     * 2.对每个小文件来进行hashmap统计，统计后再reduce对16个小文件的第一名进行统计。
     */

    // 6.32位无符号整数的范围是0~4294967295。现在有一个正好包含40亿个无符号整数的文件，所以再整个范围中必然有没有出现过的数，可以使用最多10M的内存，只要找到一个没有出现过的数即可。
    /**
     * 1.先将大文件进行分流成64个小文件。然后肯定出现某个文件没有沾满的情况，在对这个小文件进行统计。
     */

    // 7.某搜索公司一天的用户搜索词汇是百亿级别的数据量，请设计一种求出每天最热100词的可行办法。
    /**
     * 对百亿词汇进行分流，分流到不同的机器上面。然后每个机器上面进行TOP100的排序，再进行reduce进行总统的100排序。（如果第一次分流到某个机器上面后文件仍然很大，可以再进行二次hash分流成小文件进行处理。）
     */

    // 8.工程师使用服务器集群来设计和实现数据缓存，以下是常见的策略。
    //      8.1.无论是添加，删除还是修改数据，都要先将数据的id通过hash函数转换成一个hash值，即为key。
    //      8.2.如果目前机器有N台，则计算key%N的值，这个值就是该数据所属的机器编号，无论添加，删除还是查询操作，都只在这台机器上进行。请分期这种缓存策略可能带来的问题，并提出改进的方案。
    /**
     * 当机器数量变化时，进行rehash操作，此操作非常耗时，并且可能会带来stop the world行为或者执行一般的时候因为网络或硬件等原因造成中间态，从而引入新的问题。
     *
     * 一致性hash
     *
     */
}